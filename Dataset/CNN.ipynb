{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import gridspec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmux a vraiment run cette schisse.\n"
     ]
    }
   ],
   "source": [
    "print(\"Tmux a vraiment run cette schisse.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Aim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some explation about the purpose of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some insight about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Room</th>\n",
       "      <th>Array_position</th>\n",
       "      <th>Distance</th>\n",
       "      <th>Absorption</th>\n",
       "      <th>SNR</th>\n",
       "      <th>Audio_file</th>\n",
       "      <th>Phase_Matrix</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.924007832111607, 3.3174552113304, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[3.395902870077362, 2.0557500170094034, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.526185199528145, 2.2726572315160185, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>[2.6692549127850898, 3.4855723081023084, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[3.015681218831948, 1.643219467690823, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.556422618908361, 2.8311521189608246, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.616051796995994, 2.967584831240267, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>0</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.5000025446100484, 2.4977440714501156, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>[3.4733630593992753, 2.729269174981904, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>[1.7578897843542212, 1.829722126402657, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>[2.467118847207401, 1.5005407312996544, 1.5]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.702471</td>\n",
       "      <td>5</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/audio_signals...</td>\n",
       "      <td>/home/janjar/Dataset/Trainingset/phase_matrix/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Room                                 Array_position  Distance  Absorption  \\\n",
       "0     0      [1.924007832111607, 3.3174552113304, 1.5]       1.0    0.702471   \n",
       "1     0   [3.395902870077362, 2.0557500170094034, 1.5]       1.0    0.702471   \n",
       "2     0   [1.526185199528145, 2.2726572315160185, 1.5]       1.0    0.702471   \n",
       "3     0  [2.6692549127850898, 3.4855723081023084, 1.5]       1.0    0.702471   \n",
       "4     0    [3.015681218831948, 1.643219467690823, 1.5]       1.0    0.702471   \n",
       "5     0   [1.556422618908361, 2.8311521189608246, 1.5]       1.0    0.702471   \n",
       "6     0    [1.616051796995994, 2.967584831240267, 1.5]       1.0    0.702471   \n",
       "7     0  [1.5000025446100484, 2.4977440714501156, 1.5]       1.0    0.702471   \n",
       "8     0   [3.4733630593992753, 2.729269174981904, 1.5]       1.0    0.702471   \n",
       "9     0   [1.7578897843542212, 1.829722126402657, 1.5]       1.0    0.702471   \n",
       "10    0   [2.467118847207401, 1.5005407312996544, 1.5]       1.0    0.702471   \n",
       "\n",
       "   SNR                                         Audio_file  \\\n",
       "0    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "1    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "2    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "3    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "4    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "5    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "6    0  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "7    5  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "8    5  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "9    5  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "10   5  /home/janjar/Dataset/Trainingset/audio_signals...   \n",
       "\n",
       "                                         Phase_Matrix Label  \n",
       "0   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "1   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "2   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "3   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "4   /home/janjar/Dataset/Trainingset/phase_matrix/...     0  \n",
       "5   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "6   /home/janjar/Dataset/Trainingset/phase_matrix/...     1  \n",
       "7   /home/janjar/Dataset/Trainingset/phase_matrix/...     0  \n",
       "8   /home/janjar/Dataset/Trainingset/phase_matrix/...     0  \n",
       "9   /home/janjar/Dataset/Trainingset/phase_matrix/...     0  \n",
       "10  /home/janjar/Dataset/Trainingset/phase_matrix/...     0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset = pd.read_pickle(\"/home/janjar/Dataset/Trainingset/Training_dataframe.pkl\")\n",
    "Dataset.loc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining our number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rooms = Dataset.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the phase matrices in tensors with respect to the indexing shown in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 4, 129, 390)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#path containing the Training data on the machine you are working on.\n",
    "#path = \"/home/janjar/Dataset/Trainingset/\"\n",
    "\n",
    "data_matrix = np.zeros([number_of_rooms,4,129,390])\n",
    "for i in range(number_of_rooms):\n",
    "    fileName_matrix = Dataset.iloc[i]['Phase_Matrix']\n",
    "    #print(fileName_matrix)\n",
    "    fileObject2 = open(fileName_matrix, 'rb')\n",
    "    matrix_loaded = pkl.load(fileObject2)\n",
    "    fileObject2.close()\n",
    "    data_matrix[i] = matrix_loaded\n",
    "data_matrix.shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 4, 129, 390])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix = torch.from_numpy(data_matrix)\n",
    "data_matrix = data_matrix.view(-1,4,129,390)\n",
    "data_matrix = data_matrix.type('torch.FloatTensor')\n",
    "data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = torch.ones([140*390, 4,129], dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for setup in range(140):\n",
    "     for nb_tf in range(390):\n",
    "        final_data[i] = data_matrix[setup,:,:,nb_tf]\n",
    "        i = i+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54600, 4, 129])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that reshape a label into the right format for the data_targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_targets(index,label):\n",
    "    target = torch.tensor(label)\n",
    "    target = target.expand(390,1)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_targets = torch.zeros([number_of_rooms], dtype=torch.float64)\n",
    "for i in range(number_of_rooms):\n",
    "    data_targets[i] = Dataset.iloc[i]['Label']\n",
    "data_targets.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., dtype=torch.float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_targets = torch.ones([140*390], dtype=torch.float64)\n",
    "test = []\n",
    "for i in range(140):\n",
    "    curr = [data_targets[i].item()]*390\n",
    "    test.append(curr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([54600])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_targets = torch.FloatTensor(test)\n",
    "final_targets = final_targets.flatten()\n",
    "final_targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture 1.0 : vanilla CNN for DOA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this section, we will focus on building a vanilla CNN in order to recognize the directions of arrival of the   sound in a specific room. Once done, we will then compare \"by hand\" the labelel dataset to the predicted values. This is the most basic setup and will try to improve latter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self,nb_hidden = 50, n = 4):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(8 ,8, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(390096, nb_hidden)# (1x8064) being the dim of the censor obtained by flattening the output of the 3rd CL.\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(nb_hidden*4,nb_hidden)\n",
    "        self.fc3 = nn.Linear(nb_hidden,2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, n):\n",
    "        #print(\"What actually enters the model:\",x.shape)\n",
    "        test =  x[:,:,0,:,:].view(-1,1,129,390)\n",
    "        #print(\"Shape of test:\",test.shape)\n",
    "        output = torch.zeros([390096,1])\n",
    "        for i in range(n):\n",
    "            \n",
    "            y = x[:,:,i,:,:].view(-1,1,129,390)\n",
    "            y = F.relu(self.conv1(y)) \n",
    "            y = F.relu(self.conv2(y))\n",
    "            y = F.relu(self.conv3(y))\n",
    "            y = F.relu(self.fc1(y.view(-1, 390096)))\n",
    "            \n",
    "            if (i==0):\n",
    "                output = y\n",
    "            else:\n",
    "                output = torch.cat((output,y),1)\n",
    "                \n",
    "        output = F.relu(self.fc2(output))\n",
    "        output = self.dropout(output)\n",
    "        output = F.softmax(self.fc3(output))        \n",
    "\n",
    "        return output            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ws_Net(nn.Module):\n",
    "    def __init__(self,nb_hidden = 50, n = 390 ):\n",
    "        super(ws_Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=2)\n",
    "        self.conv3 = nn.Conv2d(8 ,8, kernel_size=2)\n",
    "        self.fc1 = nn.Linear(1008, nb_hidden)# (1x1008) being the dim of the censor obtained by flattening the output of the 3rd CL.\n",
    "        self.fc2 = nn.Linear(nb_hidden*n,512)\n",
    "        self.fc3 = nn.Linear(512,2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, n):\n",
    "        \n",
    "        #print(\"What actually enters the model:\",x.shape)\n",
    "        test =  x[:,:,:,:,0].view(-1,1,4,129)\n",
    "        #print(\"Shape of test:\",test.shape)\n",
    "        output = torch.zeros([1008,1])\n",
    "        \n",
    "        for i in range(n):\n",
    "            y = x[:,:,:,:,i].view(-1,1,4,129)\n",
    "            y = F.relu(self.conv1(y)) \n",
    "            y = F.relu(self.conv2(y))\n",
    "            y = F.relu(self.conv3(y))\n",
    "            y = F.relu(self.fc1(y.view(-1, 1008)))\n",
    "            \n",
    "            if (i==0):\n",
    "                output = y\n",
    "            else:\n",
    "                output = torch.cat((output,y),1)\n",
    "                \n",
    "        #print(\"Shape of the out of fc1:\",output.shape)        \n",
    "                \n",
    "        output = F.relu(self.fc2(output))\n",
    "        \n",
    "        #print(\"Shape of the out of fc2:\",output.shape) \n",
    "\n",
    "        output = F.softmax(self.fc3(output))   \n",
    "        \n",
    "        return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self, nb_hidden=100):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.cl1 = nn.Conv3d(1, 8, kernel_size=(2,2,2),stride=1)\n",
    "        self.cl2 = nn.Conv3d(8, 16, kernel_size=(2,2,2),stride=1)\n",
    "        self.cl3 = nn.Conv3d(16, 32, kernel_size=(2,2,2),stride=1)\n",
    "        self.fc1 = nn.Linear(4032, 200)\n",
    "        self.fc2 = nn.Linear(200,nb_hidden)\n",
    "        self.fc3 = nn.Linear(nb_hidden,37)\n",
    "        #self.fc4 = nn.Linear(256,37)\n",
    " \n",
    " \n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = F.relu(self.cl1(x))\n",
    "        x = F.relu(self.cl2(x))\n",
    "        x = F.relu(self.cl3(x))\n",
    "        print(\"Output of convlayer\",x.shape)\n",
    "        x = F.relu(self.fc1(x.view(-1, 4032)))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        print(\"Output of the model shape:\",x.shape)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def compute_nb_errors(self,model, data_input, data_target, mini_batch_size):\n",
    "\n",
    "        nb_data_errors = 0\n",
    "        for b in range(0, data_input.size(0), mini_batch_size):\n",
    "            a = model(data_input.narrow(0, b, mini_batch_size))\n",
    "            val = torch.max(a,1)[1]\n",
    "            for k in range(mini_batch_size):\n",
    "                if data_target.data[b + k] != val[k]:\n",
    "                    nb_data_errors = nb_data_errors + 1\n",
    "\n",
    "        return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_input_vanilla(train_input):\n",
    "    new_train_input = train_input.view(-1,1,4,129,390)\n",
    "    return new_train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_input_model2(train_input):\n",
    "    new_train_input = train_input.view(-1,1,4,129,390)\n",
    "    return new_train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 1, 4, 129, 390])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = prep_input_vanilla(data_matrix)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 1, 4, 129, 390])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = prep_input_model2(data_matrix)\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_labels_vanilla(train_input):\n",
    "    new_train_input = train_input.view(-1)\n",
    "    return new_train_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([140, 1, 129, 390])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets = prep_labels_vanilla(data_targets)\n",
    "train_targets = train_targets.to(dtype=torch.int64)\n",
    "train_data, train_targets = Variable(train_data), Variable(train_targets)\n",
    "train_data[:,:,0,:,:].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the Training Data into Training/Validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([120, 1, 4, 129, 390]),\n",
       " torch.Size([30, 1, 4, 129, 390]),\n",
       " torch.Size([120]),\n",
       " torch.Size([30]),\n",
       " 120,\n",
       " 30)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data = train_data[0:120]\n",
    "training_targets = train_targets[0:120]\n",
    "validation_data = train_data[110:140]\n",
    "validation_targets = train_targets[110:140]\n",
    "training_data.shape, validation_data.shape, training_targets.shape, validation_targets.shape,len(training_targets),len(validation_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "        0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Sanity check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ws_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_batch_size = 10\n",
    "nb_epochs = 10\n",
    "eta = 0.0001 #learning rate\n",
    "#criterion = nn.MSELoss() # MeanSquaredloss\n",
    "criterion = torch.nn.CrossEntropyLoss() #Cross Entropy\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = eta, momentum = 0.95) #Stochastic gradient descent\n",
    "optimizer1 = torch.optim.Adam(model.parameters(), lr = eta, weight_decay=0.01) #ADAM\n",
    "net_ch = 4\n",
    "ws_ch = 390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to figure out how to use mini-batches to gain time in this configuration.\n",
    "#### train_model is the method used to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_3(model, optimizer, nb_epochs, train_input, train_target ,mini_batch_size,n):\n",
    "\n",
    "    start = time.time()\n",
    "    for e in range(0,nb_epochs):\n",
    "        start_ep = time.time()\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            start = time.time()\n",
    "            #print(\"Shape of the input of the model:\",train_input.narrow(0, b, mini_batch_size).shape)\n",
    "            output = model(train_input.narrow(0, b, mini_batch_size),n)\n",
    "            target = train_target.narrow(0, b, mini_batch_size)\n",
    "            indices = output.argmax(1)\n",
    "            print(\"batch :\",b,\" Output :\",output,\"Targets:\", target)\n",
    "            loss = criterion(output,target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (e == 0):\n",
    "            end_ep = time.time()\n",
    "            print(\"Time for 1 epochs is :{:5}\".format(-(start_ep-end_ep)))\n",
    "            \n",
    "        print(\"Loss for epoch{:3} is {:5} \".format(e,loss.data.item()))\n",
    "            \n",
    "    end = time.time()\n",
    "    print(\"Time the hole training is :{:5}\".format(-(start-end)))\n",
    "\n",
    "       \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janjar/test/lib/python3.5/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 0  Output : tensor([[0.4915, 0.5085],\n",
      "        [0.4889, 0.5111],\n",
      "        [0.4861, 0.5139],\n",
      "        [0.4847, 0.5153],\n",
      "        [0.4869, 0.5131],\n",
      "        [0.4908, 0.5092],\n",
      "        [0.4909, 0.5091],\n",
      "        [0.4889, 0.5111],\n",
      "        [0.4885, 0.5115],\n",
      "        [0.4887, 0.5113]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.4030, 0.5970],\n",
      "        [0.4243, 0.5757],\n",
      "        [0.3949, 0.6051],\n",
      "        [0.3886, 0.6114],\n",
      "        [0.3996, 0.6004],\n",
      "        [0.3891, 0.6109],\n",
      "        [0.3989, 0.6011],\n",
      "        [0.3839, 0.6161],\n",
      "        [0.3892, 0.6108],\n",
      "        [0.4131, 0.5869]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.3229, 0.6771],\n",
      "        [0.3355, 0.6645],\n",
      "        [0.3397, 0.6603],\n",
      "        [0.3522, 0.6478],\n",
      "        [0.3459, 0.6541],\n",
      "        [0.3368, 0.6632],\n",
      "        [0.3318, 0.6682],\n",
      "        [0.3539, 0.6461],\n",
      "        [0.3233, 0.6767],\n",
      "        [0.3322, 0.6678]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.4040, 0.5960],\n",
      "        [0.4539, 0.5461],\n",
      "        [0.3985, 0.6015],\n",
      "        [0.4091, 0.5909],\n",
      "        [0.3988, 0.6012],\n",
      "        [0.3934, 0.6066],\n",
      "        [0.5430, 0.4570],\n",
      "        [0.4094, 0.5906],\n",
      "        [0.4033, 0.5967],\n",
      "        [0.3993, 0.6007]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.4630, 0.5370],\n",
      "        [0.4835, 0.5165],\n",
      "        [0.4639, 0.5361],\n",
      "        [0.5036, 0.4964],\n",
      "        [0.5092, 0.4908],\n",
      "        [0.4795, 0.5205],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.4905, 0.5095],\n",
      "        [0.4792, 0.5208],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.4323, 0.5677],\n",
      "        [0.5227, 0.4773],\n",
      "        [0.3958, 0.6042],\n",
      "        [0.5310, 0.4690],\n",
      "        [0.5236, 0.4764],\n",
      "        [0.5110, 0.4890],\n",
      "        [0.4495, 0.5505],\n",
      "        [0.5670, 0.4330],\n",
      "        [0.4355, 0.5645],\n",
      "        [0.5300, 0.4700]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5391, 0.4609],\n",
      "        [0.5755, 0.4245],\n",
      "        [0.5748, 0.4252],\n",
      "        [0.4802, 0.5198],\n",
      "        [0.5272, 0.4728],\n",
      "        [0.5550, 0.4450],\n",
      "        [0.5073, 0.4927],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.6017, 0.3983],\n",
      "        [0.4638, 0.5362]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5008, 0.4992],\n",
      "        [0.5556, 0.4444],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5080, 0.4920],\n",
      "        [0.5648, 0.4352],\n",
      "        [0.5555, 0.4445],\n",
      "        [0.5933, 0.4067],\n",
      "        [0.4700, 0.5300],\n",
      "        [0.4646, 0.5354],\n",
      "        [0.5843, 0.4157]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.6269, 0.3731],\n",
      "        [0.5771, 0.4229],\n",
      "        [0.5614, 0.4386],\n",
      "        [0.5326, 0.4674],\n",
      "        [0.4678, 0.5322],\n",
      "        [0.5370, 0.4630],\n",
      "        [0.5947, 0.4053],\n",
      "        [0.5872, 0.4128],\n",
      "        [0.4972, 0.5028],\n",
      "        [0.5150, 0.4850]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5243, 0.4757],\n",
      "        [0.5797, 0.4203],\n",
      "        [0.6675, 0.3325],\n",
      "        [0.4935, 0.5065],\n",
      "        [0.6026, 0.3974],\n",
      "        [0.5880, 0.4120],\n",
      "        [0.5448, 0.4552],\n",
      "        [0.5784, 0.4216],\n",
      "        [0.5652, 0.4348],\n",
      "        [0.4773, 0.5227]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.4858, 0.5142],\n",
      "        [0.5642, 0.4358],\n",
      "        [0.5392, 0.4608],\n",
      "        [0.6541, 0.3459],\n",
      "        [0.5106, 0.4894],\n",
      "        [0.3980, 0.6020],\n",
      "        [0.5926, 0.4074],\n",
      "        [0.5747, 0.4253],\n",
      "        [0.4965, 0.5035],\n",
      "        [0.5186, 0.4814]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.5982, 0.4018],\n",
      "        [0.4579, 0.5421],\n",
      "        [0.6115, 0.3885],\n",
      "        [0.5944, 0.4056],\n",
      "        [0.5926, 0.4074],\n",
      "        [0.6132, 0.3868],\n",
      "        [0.5301, 0.4699],\n",
      "        [0.6565, 0.3435],\n",
      "        [0.6174, 0.3826],\n",
      "        [0.6212, 0.3788]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Time for 1 epochs is :8.342041730880737\n",
      "Loss for epoch  0 is 0.7043451070785522 \n",
      "batch : 0  Output : tensor([[0.6212, 0.3788],\n",
      "        [0.5094, 0.4906],\n",
      "        [0.6067, 0.3933],\n",
      "        [0.3731, 0.6269],\n",
      "        [0.6406, 0.3594],\n",
      "        [0.5984, 0.4016],\n",
      "        [0.6214, 0.3786],\n",
      "        [0.5885, 0.4115],\n",
      "        [0.5987, 0.4013],\n",
      "        [0.4902, 0.5098]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.6165, 0.3835],\n",
      "        [0.6394, 0.3606],\n",
      "        [0.5325, 0.4675],\n",
      "        [0.5269, 0.4731],\n",
      "        [0.6890, 0.3110],\n",
      "        [0.5419, 0.4581],\n",
      "        [0.5517, 0.4483],\n",
      "        [0.5009, 0.4991],\n",
      "        [0.5907, 0.4093],\n",
      "        [0.6782, 0.3218]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.6532, 0.3468],\n",
      "        [0.7082, 0.2918],\n",
      "        [0.5231, 0.4769],\n",
      "        [0.5592, 0.4408],\n",
      "        [0.5357, 0.4643],\n",
      "        [0.5718, 0.4282],\n",
      "        [0.6097, 0.3903],\n",
      "        [0.6377, 0.3623],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5773, 0.4227]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.6275, 0.3725],\n",
      "        [0.6058, 0.3942],\n",
      "        [0.6503, 0.3497],\n",
      "        [0.5749, 0.4251],\n",
      "        [0.5197, 0.4803],\n",
      "        [0.6128, 0.3872],\n",
      "        [0.7177, 0.2823],\n",
      "        [0.6452, 0.3548],\n",
      "        [0.6387, 0.3613],\n",
      "        [0.4839, 0.5161]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.6007, 0.3993],\n",
      "        [0.6161, 0.3839],\n",
      "        [0.6106, 0.3894],\n",
      "        [0.6149, 0.3851],\n",
      "        [0.6591, 0.3409],\n",
      "        [0.5985, 0.4015],\n",
      "        [0.6567, 0.3433],\n",
      "        [0.5969, 0.4031],\n",
      "        [0.5342, 0.4658],\n",
      "        [0.4990, 0.5010]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5969, 0.4031],\n",
      "        [0.6411, 0.3589],\n",
      "        [0.5579, 0.4421],\n",
      "        [0.6333, 0.3667],\n",
      "        [0.6610, 0.3390],\n",
      "        [0.6060, 0.3940],\n",
      "        [0.6163, 0.3837],\n",
      "        [0.5308, 0.4692],\n",
      "        [0.4247, 0.5753],\n",
      "        [0.5487, 0.4513]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.6069, 0.3931],\n",
      "        [0.5396, 0.4604],\n",
      "        [0.5480, 0.4520],\n",
      "        [0.5262, 0.4738],\n",
      "        [0.6414, 0.3586],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.4842, 0.5158],\n",
      "        [0.6162, 0.3838],\n",
      "        [0.5620, 0.4380],\n",
      "        [0.6632, 0.3368]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5784, 0.4216],\n",
      "        [0.6222, 0.3778],\n",
      "        [0.6036, 0.3964],\n",
      "        [0.4412, 0.5588],\n",
      "        [0.6703, 0.3297],\n",
      "        [0.4972, 0.5028],\n",
      "        [0.6580, 0.3420],\n",
      "        [0.5603, 0.4397],\n",
      "        [0.6042, 0.3958],\n",
      "        [0.5537, 0.4463]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5755, 0.4245],\n",
      "        [0.6666, 0.3334],\n",
      "        [0.4386, 0.5614],\n",
      "        [0.5626, 0.4374],\n",
      "        [0.6097, 0.3903],\n",
      "        [0.6487, 0.3513],\n",
      "        [0.6085, 0.3915],\n",
      "        [0.5538, 0.4462],\n",
      "        [0.6080, 0.3920],\n",
      "        [0.4652, 0.5348]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5433, 0.4567],\n",
      "        [0.6389, 0.3611],\n",
      "        [0.5917, 0.4083],\n",
      "        [0.6733, 0.3267],\n",
      "        [0.6078, 0.3922],\n",
      "        [0.6501, 0.3499],\n",
      "        [0.5297, 0.4703],\n",
      "        [0.6255, 0.3745],\n",
      "        [0.7236, 0.2764],\n",
      "        [0.5952, 0.4048]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.6250, 0.3750],\n",
      "        [0.4734, 0.5266],\n",
      "        [0.6297, 0.3703],\n",
      "        [0.6292, 0.3708],\n",
      "        [0.4103, 0.5897],\n",
      "        [0.7706, 0.2294],\n",
      "        [0.5577, 0.4423],\n",
      "        [0.6613, 0.3387],\n",
      "        [0.6280, 0.3720],\n",
      "        [0.6829, 0.3171]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 110  Output : tensor([[0.6738, 0.3262],\n",
      "        [0.6302, 0.3698],\n",
      "        [0.5922, 0.4078],\n",
      "        [0.6145, 0.3855],\n",
      "        [0.6456, 0.3544],\n",
      "        [0.6340, 0.3660],\n",
      "        [0.6562, 0.3438],\n",
      "        [0.5602, 0.4398],\n",
      "        [0.5623, 0.4377],\n",
      "        [0.5691, 0.4309]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  1 is 0.6667132377624512 \n",
      "batch : 0  Output : tensor([[0.3960, 0.6040],\n",
      "        [0.6199, 0.3801],\n",
      "        [0.5498, 0.4502],\n",
      "        [0.4959, 0.5041],\n",
      "        [0.7630, 0.2370],\n",
      "        [0.4959, 0.5041],\n",
      "        [0.4568, 0.5432],\n",
      "        [0.6933, 0.3067],\n",
      "        [0.7178, 0.2822],\n",
      "        [0.7026, 0.2974]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.6461, 0.3539],\n",
      "        [0.6878, 0.3122],\n",
      "        [0.6774, 0.3226],\n",
      "        [0.7459, 0.2541],\n",
      "        [0.6993, 0.3007],\n",
      "        [0.5431, 0.4569],\n",
      "        [0.6265, 0.3735],\n",
      "        [0.7410, 0.2590],\n",
      "        [0.5970, 0.4030],\n",
      "        [0.5980, 0.4020]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.3651, 0.6349],\n",
      "        [0.5867, 0.4133],\n",
      "        [0.6941, 0.3059],\n",
      "        [0.5290, 0.4710],\n",
      "        [0.6232, 0.3768],\n",
      "        [0.6182, 0.3818],\n",
      "        [0.5203, 0.4797],\n",
      "        [0.7505, 0.2495],\n",
      "        [0.6593, 0.3407],\n",
      "        [0.4191, 0.5809]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5457, 0.4543],\n",
      "        [0.6692, 0.3308],\n",
      "        [0.5930, 0.4070],\n",
      "        [0.4181, 0.5819],\n",
      "        [0.6371, 0.3629],\n",
      "        [0.7210, 0.2790],\n",
      "        [0.6505, 0.3495],\n",
      "        [0.6712, 0.3288],\n",
      "        [0.6591, 0.3409],\n",
      "        [0.6891, 0.3109]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5990, 0.4010],\n",
      "        [0.6450, 0.3550],\n",
      "        [0.7225, 0.2775],\n",
      "        [0.6611, 0.3389],\n",
      "        [0.7493, 0.2507],\n",
      "        [0.6814, 0.3186],\n",
      "        [0.5620, 0.4380],\n",
      "        [0.5573, 0.4427],\n",
      "        [0.5647, 0.4353],\n",
      "        [0.5257, 0.4743]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.7668, 0.2332],\n",
      "        [0.6436, 0.3564],\n",
      "        [0.6920, 0.3080],\n",
      "        [0.7224, 0.2776],\n",
      "        [0.6662, 0.3338],\n",
      "        [0.6738, 0.3262],\n",
      "        [0.3806, 0.6194],\n",
      "        [0.7161, 0.2839],\n",
      "        [0.6805, 0.3195],\n",
      "        [0.6161, 0.3839]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.6088, 0.3912],\n",
      "        [0.6661, 0.3339],\n",
      "        [0.6752, 0.3248],\n",
      "        [0.6774, 0.3226],\n",
      "        [0.6942, 0.3058],\n",
      "        [0.6891, 0.3109],\n",
      "        [0.4967, 0.5033],\n",
      "        [0.7588, 0.2412],\n",
      "        [0.6854, 0.3146],\n",
      "        [0.7211, 0.2789]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5259, 0.4741],\n",
      "        [0.7022, 0.2978],\n",
      "        [0.7622, 0.2378],\n",
      "        [0.7178, 0.2822],\n",
      "        [0.6820, 0.3180],\n",
      "        [0.5501, 0.4499],\n",
      "        [0.7091, 0.2909],\n",
      "        [0.8008, 0.1992],\n",
      "        [0.7358, 0.2642],\n",
      "        [0.7204, 0.2796]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.6278, 0.3722],\n",
      "        [0.7912, 0.2088],\n",
      "        [0.4620, 0.5380],\n",
      "        [0.6490, 0.3510],\n",
      "        [0.6490, 0.3510],\n",
      "        [0.7214, 0.2786],\n",
      "        [0.6137, 0.3863],\n",
      "        [0.5683, 0.4317],\n",
      "        [0.7093, 0.2907],\n",
      "        [0.5334, 0.4666]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5702, 0.4298],\n",
      "        [0.7936, 0.2064],\n",
      "        [0.5101, 0.4899],\n",
      "        [0.6953, 0.3047],\n",
      "        [0.4686, 0.5314],\n",
      "        [0.7223, 0.2777],\n",
      "        [0.7579, 0.2421],\n",
      "        [0.7075, 0.2925],\n",
      "        [0.6080, 0.3920],\n",
      "        [0.5747, 0.4253]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.7014, 0.2986],\n",
      "        [0.4827, 0.5173],\n",
      "        [0.5659, 0.4341],\n",
      "        [0.6324, 0.3676],\n",
      "        [0.4634, 0.5366],\n",
      "        [0.7245, 0.2755],\n",
      "        [0.5676, 0.4324],\n",
      "        [0.8176, 0.1824],\n",
      "        [0.7929, 0.2071],\n",
      "        [0.7832, 0.2168]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.7484, 0.2516],\n",
      "        [0.7494, 0.2506],\n",
      "        [0.4521, 0.5479],\n",
      "        [0.4492, 0.5508],\n",
      "        [0.7528, 0.2472],\n",
      "        [0.5508, 0.4492],\n",
      "        [0.7176, 0.2824],\n",
      "        [0.5182, 0.4818],\n",
      "        [0.7217, 0.2783],\n",
      "        [0.7423, 0.2577]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  2 is 0.5643250942230225 \n",
      "batch : 0  Output : tensor([[0.4209, 0.5791],\n",
      "        [0.4700, 0.5300],\n",
      "        [0.2537, 0.7463],\n",
      "        [0.4598, 0.5402],\n",
      "        [0.7133, 0.2867],\n",
      "        [0.3513, 0.6487],\n",
      "        [0.4279, 0.5721],\n",
      "        [0.7561, 0.2439],\n",
      "        [0.7374, 0.2626],\n",
      "        [0.4467, 0.5533]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.7268, 0.2732],\n",
      "        [0.7382, 0.2618],\n",
      "        [0.2681, 0.7319],\n",
      "        [0.8191, 0.1809],\n",
      "        [0.7407, 0.2593],\n",
      "        [0.3858, 0.6142],\n",
      "        [0.4433, 0.5567],\n",
      "        [0.6803, 0.3197],\n",
      "        [0.4074, 0.5926],\n",
      "        [0.4876, 0.5124]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5571, 0.4429],\n",
      "        [0.7613, 0.2387],\n",
      "        [0.7878, 0.2122],\n",
      "        [0.5557, 0.4443],\n",
      "        [0.6467, 0.3533],\n",
      "        [0.6684, 0.3316],\n",
      "        [0.6384, 0.3616],\n",
      "        [0.6726, 0.3274],\n",
      "        [0.5611, 0.4389],\n",
      "        [0.4409, 0.5591]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5343, 0.4657],\n",
      "        [0.5426, 0.4574],\n",
      "        [0.7215, 0.2785],\n",
      "        [0.3844, 0.6156],\n",
      "        [0.6159, 0.3841],\n",
      "        [0.4445, 0.5555],\n",
      "        [0.6722, 0.3278],\n",
      "        [0.6825, 0.3175],\n",
      "        [0.7218, 0.2782],\n",
      "        [0.6668, 0.3332]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.2056, 0.7944],\n",
      "        [0.7730, 0.2270],\n",
      "        [0.6392, 0.3608],\n",
      "        [0.8360, 0.1640],\n",
      "        [0.6963, 0.3037],\n",
      "        [0.7757, 0.2243],\n",
      "        [0.7880, 0.2120],\n",
      "        [0.4936, 0.5064],\n",
      "        [0.2433, 0.7567],\n",
      "        [0.3247, 0.6753]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.7063, 0.2937],\n",
      "        [0.4760, 0.5240],\n",
      "        [0.3896, 0.6104],\n",
      "        [0.7372, 0.2628],\n",
      "        [0.7319, 0.2681],\n",
      "        [0.6790, 0.3210],\n",
      "        [0.3454, 0.6546],\n",
      "        [0.3956, 0.6044],\n",
      "        [0.6901, 0.3099],\n",
      "        [0.5028, 0.4972]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.4862, 0.5138],\n",
      "        [0.4636, 0.5364],\n",
      "        [0.7757, 0.2243],\n",
      "        [0.6897, 0.3103],\n",
      "        [0.6406, 0.3594],\n",
      "        [0.4592, 0.5408],\n",
      "        [0.4280, 0.5720],\n",
      "        [0.8786, 0.1214],\n",
      "        [0.5898, 0.4102],\n",
      "        [0.6928, 0.3072]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5442, 0.4558],\n",
      "        [0.6125, 0.3875],\n",
      "        [0.7390, 0.2610],\n",
      "        [0.8180, 0.1820],\n",
      "        [0.8399, 0.1601],\n",
      "        [0.2432, 0.7568],\n",
      "        [0.7922, 0.2078],\n",
      "        [0.7107, 0.2893],\n",
      "        [0.8042, 0.1958],\n",
      "        [0.7801, 0.2199]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.4784, 0.5216],\n",
      "        [0.7868, 0.2132],\n",
      "        [0.2526, 0.7474],\n",
      "        [0.5564, 0.4436],\n",
      "        [0.8022, 0.1978],\n",
      "        [0.8306, 0.1694],\n",
      "        [0.8070, 0.1930],\n",
      "        [0.3410, 0.6590],\n",
      "        [0.6824, 0.3176],\n",
      "        [0.3820, 0.6180]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.3548, 0.6452],\n",
      "        [0.7939, 0.2061],\n",
      "        [0.4945, 0.5055],\n",
      "        [0.6762, 0.3238],\n",
      "        [0.3801, 0.6199],\n",
      "        [0.7447, 0.2553],\n",
      "        [0.8363, 0.1637],\n",
      "        [0.6711, 0.3289],\n",
      "        [0.8926, 0.1074],\n",
      "        [0.4391, 0.5609]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 100  Output : tensor([[0.6960, 0.3040],\n",
      "        [0.2542, 0.7458],\n",
      "        [0.8404, 0.1596],\n",
      "        [0.4436, 0.5564],\n",
      "        [0.5588, 0.4412],\n",
      "        [0.8164, 0.1836],\n",
      "        [0.3008, 0.6992],\n",
      "        [0.8185, 0.1815],\n",
      "        [0.7461, 0.2539],\n",
      "        [0.8640, 0.1360]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.7525, 0.2475],\n",
      "        [0.7317, 0.2683],\n",
      "        [0.2205, 0.7795],\n",
      "        [0.6252, 0.3748],\n",
      "        [0.8513, 0.1487],\n",
      "        [0.3457, 0.6543],\n",
      "        [0.8424, 0.1576],\n",
      "        [0.2871, 0.7129],\n",
      "        [0.6739, 0.3261],\n",
      "        [0.8983, 0.1017]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  3 is 0.5009082555770874 \n",
      "batch : 0  Output : tensor([[0.6643, 0.3357],\n",
      "        [0.1930, 0.8070],\n",
      "        [0.3663, 0.6337],\n",
      "        [0.2908, 0.7092],\n",
      "        [0.7087, 0.2913],\n",
      "        [0.2660, 0.7340],\n",
      "        [0.3405, 0.6595],\n",
      "        [0.8335, 0.1665],\n",
      "        [0.8651, 0.1349],\n",
      "        [0.7220, 0.2780]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.8111, 0.1889],\n",
      "        [0.7634, 0.2366],\n",
      "        [0.6029, 0.3971],\n",
      "        [0.7380, 0.2620],\n",
      "        [0.6868, 0.3132],\n",
      "        [0.4233, 0.5767],\n",
      "        [0.2078, 0.7922],\n",
      "        [0.7179, 0.2821],\n",
      "        [0.2207, 0.7793],\n",
      "        [0.2704, 0.7296]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.3956, 0.6044],\n",
      "        [0.7460, 0.2540],\n",
      "        [0.8516, 0.1484],\n",
      "        [0.1760, 0.8240],\n",
      "        [0.6338, 0.3662],\n",
      "        [0.7091, 0.2909],\n",
      "        [0.5512, 0.4488],\n",
      "        [0.5090, 0.4910],\n",
      "        [0.8126, 0.1874],\n",
      "        [0.3147, 0.6853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.3174, 0.6826],\n",
      "        [0.8191, 0.1809],\n",
      "        [0.6336, 0.3664],\n",
      "        [0.4849, 0.5151],\n",
      "        [0.5825, 0.4175],\n",
      "        [0.2746, 0.7254],\n",
      "        [0.7171, 0.2829],\n",
      "        [0.5878, 0.4122],\n",
      "        [0.8301, 0.1699],\n",
      "        [0.7789, 0.2211]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.2025, 0.7975],\n",
      "        [0.8669, 0.1331],\n",
      "        [0.5059, 0.4941],\n",
      "        [0.7948, 0.2052],\n",
      "        [0.7057, 0.2943],\n",
      "        [0.8710, 0.1290],\n",
      "        [0.5975, 0.4025],\n",
      "        [0.4235, 0.5765],\n",
      "        [0.1347, 0.8653],\n",
      "        [0.1937, 0.8063]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.8470, 0.1530],\n",
      "        [0.3986, 0.6014],\n",
      "        [0.5011, 0.4989],\n",
      "        [0.8310, 0.1690],\n",
      "        [0.5784, 0.4216],\n",
      "        [0.8581, 0.1419],\n",
      "        [0.1629, 0.8371],\n",
      "        [0.4208, 0.5792],\n",
      "        [0.3598, 0.6402],\n",
      "        [0.1949, 0.8051]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.3738, 0.6262],\n",
      "        [0.7245, 0.2755],\n",
      "        [0.7169, 0.2831],\n",
      "        [0.6770, 0.3230],\n",
      "        [0.7268, 0.2732],\n",
      "        [0.2955, 0.7045],\n",
      "        [0.5332, 0.4668],\n",
      "        [0.7818, 0.2182],\n",
      "        [0.6190, 0.3810],\n",
      "        [0.8066, 0.1934]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.1232, 0.8768],\n",
      "        [0.8382, 0.1618],\n",
      "        [0.8560, 0.1440],\n",
      "        [0.7906, 0.2094],\n",
      "        [0.8899, 0.1101],\n",
      "        [0.1685, 0.8315],\n",
      "        [0.8462, 0.1538],\n",
      "        [0.6995, 0.3005],\n",
      "        [0.7713, 0.2287],\n",
      "        [0.8596, 0.1404]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.1992, 0.8008],\n",
      "        [0.7196, 0.2804],\n",
      "        [0.2069, 0.7931],\n",
      "        [0.3779, 0.6221],\n",
      "        [0.7462, 0.2538],\n",
      "        [0.6803, 0.3197],\n",
      "        [0.7865, 0.2135],\n",
      "        [0.1356, 0.8644],\n",
      "        [0.8613, 0.1387],\n",
      "        [0.3304, 0.6696]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.2664, 0.7336],\n",
      "        [0.8777, 0.1223],\n",
      "        [0.2540, 0.7460],\n",
      "        [0.7891, 0.2109],\n",
      "        [0.3996, 0.6004],\n",
      "        [0.8999, 0.1001],\n",
      "        [0.7484, 0.2516],\n",
      "        [0.8040, 0.1960],\n",
      "        [0.8942, 0.1058],\n",
      "        [0.3058, 0.6942]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.7861, 0.2139],\n",
      "        [0.3517, 0.6483],\n",
      "        [0.8924, 0.1076],\n",
      "        [0.2888, 0.7112],\n",
      "        [0.3954, 0.6046],\n",
      "        [0.9251, 0.0749],\n",
      "        [0.2703, 0.7297],\n",
      "        [0.7784, 0.2216],\n",
      "        [0.9143, 0.0857],\n",
      "        [0.9224, 0.0776]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.8717, 0.1283],\n",
      "        [0.8582, 0.1418],\n",
      "        [0.2968, 0.7032],\n",
      "        [0.1623, 0.8377],\n",
      "        [0.8766, 0.1234],\n",
      "        [0.2018, 0.7982],\n",
      "        [0.7859, 0.2141],\n",
      "        [0.3622, 0.6378],\n",
      "        [0.9306, 0.0694],\n",
      "        [0.8917, 0.1083]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  4 is 0.4274412989616394 \n",
      "batch : 0  Output : tensor([[0.3062, 0.6938],\n",
      "        [0.3358, 0.6642],\n",
      "        [0.1897, 0.8103],\n",
      "        [0.2294, 0.7706],\n",
      "        [0.8967, 0.1033],\n",
      "        [0.1217, 0.8783],\n",
      "        [0.1637, 0.8363],\n",
      "        [0.9152, 0.0848],\n",
      "        [0.9391, 0.0609],\n",
      "        [0.8427, 0.1573]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.8924, 0.1076],\n",
      "        [0.9311, 0.0689],\n",
      "        [0.3103, 0.6897],\n",
      "        [0.9171, 0.0829],\n",
      "        [0.7998, 0.2002],\n",
      "        [0.1679, 0.8321],\n",
      "        [0.3376, 0.6624],\n",
      "        [0.6527, 0.3473],\n",
      "        [0.3428, 0.6572],\n",
      "        [0.2813, 0.7187]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.4841, 0.5159],\n",
      "        [0.8615, 0.1385],\n",
      "        [0.8809, 0.1191],\n",
      "        [0.2067, 0.7933],\n",
      "        [0.6207, 0.3793],\n",
      "        [0.9110, 0.0890],\n",
      "        [0.7214, 0.2786],\n",
      "        [0.8647, 0.1353],\n",
      "        [0.8082, 0.1918],\n",
      "        [0.2035, 0.7965]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.4029, 0.5971],\n",
      "        [0.8598, 0.1402],\n",
      "        [0.8177, 0.1823],\n",
      "        [0.1265, 0.8735],\n",
      "        [0.6549, 0.3451],\n",
      "        [0.0726, 0.9274],\n",
      "        [0.9125, 0.0875],\n",
      "        [0.9213, 0.0787],\n",
      "        [0.8773, 0.1227],\n",
      "        [0.8479, 0.1521]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.0999, 0.9001],\n",
      "        [0.9067, 0.0933],\n",
      "        [0.6503, 0.3497],\n",
      "        [0.8932, 0.1068],\n",
      "        [0.8838, 0.1162],\n",
      "        [0.8585, 0.1415],\n",
      "        [0.8825, 0.1175],\n",
      "        [0.1459, 0.8541],\n",
      "        [0.4604, 0.5396],\n",
      "        [0.0790, 0.9210]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.9337, 0.0663],\n",
      "        [0.3628, 0.6372],\n",
      "        [0.1690, 0.8310],\n",
      "        [0.9024, 0.0976],\n",
      "        [0.9259, 0.0741],\n",
      "        [0.6790, 0.3210],\n",
      "        [0.1311, 0.8689],\n",
      "        [0.2226, 0.7774],\n",
      "        [0.3711, 0.6289],\n",
      "        [0.1450, 0.8550]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.2090, 0.7910],\n",
      "        [0.7288, 0.2712],\n",
      "        [0.7830, 0.2170],\n",
      "        [0.8446, 0.1554],\n",
      "        [0.7841, 0.2159],\n",
      "        [0.1543, 0.8457],\n",
      "        [0.2813, 0.7187],\n",
      "        [0.9075, 0.0925],\n",
      "        [0.8465, 0.1535],\n",
      "        [0.7683, 0.2317]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.2757, 0.7243],\n",
      "        [0.8529, 0.1471],\n",
      "        [0.8855, 0.1145],\n",
      "        [0.7656, 0.2344],\n",
      "        [0.7590, 0.2410],\n",
      "        [0.1260, 0.8740],\n",
      "        [0.8620, 0.1380],\n",
      "        [0.8492, 0.1508],\n",
      "        [0.9024, 0.0976],\n",
      "        [0.8402, 0.1598]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.1264, 0.8736],\n",
      "        [0.8987, 0.1013],\n",
      "        [0.1320, 0.8680],\n",
      "        [0.0817, 0.9183],\n",
      "        [0.6726, 0.3274],\n",
      "        [0.8698, 0.1302],\n",
      "        [0.3526, 0.6474],\n",
      "        [0.0635, 0.9365],\n",
      "        [0.8906, 0.1094],\n",
      "        [0.0427, 0.9573]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 90  Output : tensor([[0.0565, 0.9435],\n",
      "        [0.6931, 0.3069],\n",
      "        [0.0865, 0.9135],\n",
      "        [0.8132, 0.1868],\n",
      "        [0.1264, 0.8736],\n",
      "        [0.8968, 0.1032],\n",
      "        [0.8653, 0.1347],\n",
      "        [0.9359, 0.0641],\n",
      "        [0.8926, 0.1074],\n",
      "        [0.0784, 0.9216]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.9288, 0.0712],\n",
      "        [0.0650, 0.9350],\n",
      "        [0.7941, 0.2059],\n",
      "        [0.1338, 0.8662],\n",
      "        [0.1672, 0.8328],\n",
      "        [0.9172, 0.0828],\n",
      "        [0.0707, 0.9293],\n",
      "        [0.9323, 0.0677],\n",
      "        [0.8387, 0.1613],\n",
      "        [0.9363, 0.0637]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.9438, 0.0562],\n",
      "        [0.9707, 0.0293],\n",
      "        [0.2530, 0.7470],\n",
      "        [0.0617, 0.9383],\n",
      "        [0.9499, 0.0501],\n",
      "        [0.1388, 0.8612],\n",
      "        [0.8780, 0.1220],\n",
      "        [0.1047, 0.8953],\n",
      "        [0.9668, 0.0332],\n",
      "        [0.9338, 0.0662]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  5 is 0.36765849590301514 \n",
      "batch : 0  Output : tensor([[0.2153, 0.7847],\n",
      "        [0.1952, 0.8048],\n",
      "        [0.3620, 0.6380],\n",
      "        [0.4299, 0.5701],\n",
      "        [0.9555, 0.0445],\n",
      "        [0.2674, 0.7326],\n",
      "        [0.1865, 0.8135],\n",
      "        [0.7976, 0.2024],\n",
      "        [0.9390, 0.0610],\n",
      "        [0.9198, 0.0802]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.9534, 0.0466],\n",
      "        [0.9417, 0.0583],\n",
      "        [0.2367, 0.7633],\n",
      "        [0.9483, 0.0517],\n",
      "        [0.9591, 0.0409],\n",
      "        [0.1592, 0.8408],\n",
      "        [0.2092, 0.7908],\n",
      "        [0.8282, 0.1718],\n",
      "        [0.1934, 0.8066],\n",
      "        [0.2739, 0.7261]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.1666, 0.8334],\n",
      "        [0.9494, 0.0506],\n",
      "        [0.9359, 0.0641],\n",
      "        [0.1372, 0.8628],\n",
      "        [0.9782, 0.0218],\n",
      "        [0.8767, 0.1233],\n",
      "        [0.7826, 0.2174],\n",
      "        [0.8624, 0.1376],\n",
      "        [0.9364, 0.0636],\n",
      "        [0.0781, 0.9219]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.1504, 0.8496],\n",
      "        [0.9052, 0.0948],\n",
      "        [0.9335, 0.0665],\n",
      "        [0.1840, 0.8160],\n",
      "        [0.8851, 0.1149],\n",
      "        [0.2874, 0.7126],\n",
      "        [0.9691, 0.0309],\n",
      "        [0.8577, 0.1423],\n",
      "        [0.9334, 0.0666],\n",
      "        [0.9135, 0.0865]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.4889, 0.5111],\n",
      "        [0.9443, 0.0557],\n",
      "        [0.7500, 0.2500],\n",
      "        [0.9398, 0.0602],\n",
      "        [0.9470, 0.0530],\n",
      "        [0.9548, 0.0452],\n",
      "        [0.9297, 0.0703],\n",
      "        [0.1760, 0.8240],\n",
      "        [0.0437, 0.9563],\n",
      "        [0.1159, 0.8841]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.9532, 0.0468],\n",
      "        [0.2332, 0.7668],\n",
      "        [0.1551, 0.8449],\n",
      "        [0.9388, 0.0612],\n",
      "        [0.9248, 0.0752],\n",
      "        [0.8688, 0.1312],\n",
      "        [0.1273, 0.8727],\n",
      "        [0.2891, 0.7109],\n",
      "        [0.1717, 0.8283],\n",
      "        [0.2058, 0.7942]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.1521, 0.8479],\n",
      "        [0.8852, 0.1148],\n",
      "        [0.9027, 0.0973],\n",
      "        [0.9340, 0.0660],\n",
      "        [0.9093, 0.0907],\n",
      "        [0.2145, 0.7855],\n",
      "        [0.2239, 0.7761],\n",
      "        [0.8538, 0.1462],\n",
      "        [0.9216, 0.0784],\n",
      "        [0.9332, 0.0668]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.0223, 0.9777],\n",
      "        [0.9271, 0.0729],\n",
      "        [0.9158, 0.0842],\n",
      "        [0.9056, 0.0944],\n",
      "        [0.8772, 0.1228],\n",
      "        [0.0326, 0.9674],\n",
      "        [0.9318, 0.0682],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.9116, 0.0884],\n",
      "        [0.8566, 0.1434]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.0263, 0.9737],\n",
      "        [0.8336, 0.1664],\n",
      "        [0.0402, 0.9598],\n",
      "        [0.0608, 0.9392],\n",
      "        [0.9044, 0.0956],\n",
      "        [0.9287, 0.0713],\n",
      "        [0.8041, 0.1959],\n",
      "        [0.0625, 0.9375],\n",
      "        [0.8350, 0.1650],\n",
      "        [0.0302, 0.9698]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.0396, 0.9604],\n",
      "        [0.8849, 0.1151],\n",
      "        [0.0164, 0.9836],\n",
      "        [0.9431, 0.0569],\n",
      "        [0.0395, 0.9605],\n",
      "        [0.9112, 0.0888],\n",
      "        [0.7935, 0.2065],\n",
      "        [0.8886, 0.1114],\n",
      "        [0.8143, 0.1857],\n",
      "        [0.0572, 0.9428]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.8605, 0.1395],\n",
      "        [0.0483, 0.9517],\n",
      "        [0.9318, 0.0682],\n",
      "        [0.0207, 0.9793],\n",
      "        [0.0202, 0.9798],\n",
      "        [0.8793, 0.1207],\n",
      "        [0.0596, 0.9404],\n",
      "        [0.7419, 0.2581],\n",
      "        [0.9236, 0.0764],\n",
      "        [0.8585, 0.1415]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.9392, 0.0608],\n",
      "        [0.9411, 0.0589],\n",
      "        [0.0282, 0.9718],\n",
      "        [0.0503, 0.9497],\n",
      "        [0.7068, 0.2932],\n",
      "        [0.0178, 0.9822],\n",
      "        [0.8733, 0.1267],\n",
      "        [0.0459, 0.9541],\n",
      "        [0.9430, 0.0570],\n",
      "        [0.6925, 0.3075]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  6 is 0.3785456120967865 \n",
      "batch : 0  Output : tensor([[0.0836, 0.9164],\n",
      "        [0.1050, 0.8950],\n",
      "        [0.0350, 0.9650],\n",
      "        [0.0150, 0.9850],\n",
      "        [0.9634, 0.0366],\n",
      "        [0.0253, 0.9747],\n",
      "        [0.0399, 0.9601],\n",
      "        [0.7990, 0.2010],\n",
      "        [0.8726, 0.1274],\n",
      "        [0.9180, 0.0820]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.9629, 0.0371],\n",
      "        [0.8807, 0.1193],\n",
      "        [0.0158, 0.9842],\n",
      "        [0.8845, 0.1155],\n",
      "        [0.9638, 0.0362],\n",
      "        [0.0432, 0.9568],\n",
      "        [0.0279, 0.9721],\n",
      "        [0.8488, 0.1512],\n",
      "        [0.0946, 0.9054],\n",
      "        [0.0268, 0.9732]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.0576, 0.9424],\n",
      "        [0.9600, 0.0400],\n",
      "        [0.9227, 0.0773],\n",
      "        [0.1416, 0.8584],\n",
      "        [0.9668, 0.0332],\n",
      "        [0.9283, 0.0717],\n",
      "        [0.8526, 0.1474],\n",
      "        [0.7341, 0.2659],\n",
      "        [0.9877, 0.0123],\n",
      "        [0.0804, 0.9196]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.2825, 0.7175],\n",
      "        [0.9407, 0.0593],\n",
      "        [0.9790, 0.0210],\n",
      "        [0.2232, 0.7768],\n",
      "        [0.7202, 0.2798],\n",
      "        [0.0988, 0.9012],\n",
      "        [0.9559, 0.0441],\n",
      "        [0.9421, 0.0579],\n",
      "        [0.9825, 0.0175],\n",
      "        [0.9681, 0.0319]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.0814, 0.9186],\n",
      "        [0.9828, 0.0172],\n",
      "        [0.9809, 0.0191],\n",
      "        [0.9883, 0.0117],\n",
      "        [0.9794, 0.0206],\n",
      "        [0.9875, 0.0125],\n",
      "        [0.9571, 0.0429],\n",
      "        [0.2732, 0.7268],\n",
      "        [0.0758, 0.9242],\n",
      "        [0.1337, 0.8663]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.9866, 0.0134],\n",
      "        [0.0882, 0.9118],\n",
      "        [0.1600, 0.8400],\n",
      "        [0.9722, 0.0278],\n",
      "        [0.9809, 0.0191],\n",
      "        [0.9755, 0.0245],\n",
      "        [0.1463, 0.8537],\n",
      "        [0.2491, 0.7509],\n",
      "        [0.4538, 0.5462],\n",
      "        [0.0406, 0.9594]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.2651, 0.7349],\n",
      "        [0.9854, 0.0146],\n",
      "        [0.9846, 0.0154],\n",
      "        [0.9746, 0.0254],\n",
      "        [0.9149, 0.0851],\n",
      "        [0.2341, 0.7659],\n",
      "        [0.2352, 0.7648],\n",
      "        [0.9902, 0.0098],\n",
      "        [0.9742, 0.0258],\n",
      "        [0.9871, 0.0129]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.0859, 0.9141],\n",
      "        [0.9908, 0.0092],\n",
      "        [0.9897, 0.0103],\n",
      "        [0.9898, 0.0102],\n",
      "        [0.9836, 0.0164],\n",
      "        [0.1344, 0.8656],\n",
      "        [0.9822, 0.0178],\n",
      "        [0.9731, 0.0269],\n",
      "        [0.9881, 0.0119],\n",
      "        [0.9782, 0.0218]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 80  Output : tensor([[0.0286, 0.9714],\n",
      "        [0.9824, 0.0176],\n",
      "        [0.0856, 0.9144],\n",
      "        [0.0742, 0.9258],\n",
      "        [0.9849, 0.0151],\n",
      "        [0.9859, 0.0141],\n",
      "        [0.9612, 0.0388],\n",
      "        [0.0655, 0.9345],\n",
      "        [0.9851, 0.0149],\n",
      "        [0.1029, 0.8971]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.0596, 0.9404],\n",
      "        [0.9798, 0.0202],\n",
      "        [0.1152, 0.8848],\n",
      "        [0.9193, 0.0807],\n",
      "        [0.1806, 0.8194],\n",
      "        [0.9837, 0.0163],\n",
      "        [0.9820, 0.0180],\n",
      "        [0.9781, 0.0219],\n",
      "        [0.9826, 0.0174],\n",
      "        [0.0742, 0.9258]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.9218, 0.0782],\n",
      "        [0.0383, 0.9617],\n",
      "        [0.9439, 0.0561],\n",
      "        [0.0519, 0.9481],\n",
      "        [0.0468, 0.9532],\n",
      "        [0.8403, 0.1597],\n",
      "        [0.0327, 0.9673],\n",
      "        [0.9528, 0.0472],\n",
      "        [0.9658, 0.0342],\n",
      "        [0.9207, 0.0793]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.8589, 0.1411],\n",
      "        [0.9556, 0.0444],\n",
      "        [0.0337, 0.9663],\n",
      "        [0.0328, 0.9672],\n",
      "        [0.9735, 0.0265],\n",
      "        [0.0268, 0.9732],\n",
      "        [0.9704, 0.0296],\n",
      "        [0.0186, 0.9814],\n",
      "        [0.9502, 0.0498],\n",
      "        [0.9809, 0.0191]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  7 is 0.33718618750572205 \n",
      "batch : 0  Output : tensor([[0.0174, 0.9826],\n",
      "        [0.0263, 0.9737],\n",
      "        [0.0326, 0.9674],\n",
      "        [0.0230, 0.9770],\n",
      "        [0.9478, 0.0522],\n",
      "        [0.0113, 0.9887],\n",
      "        [0.0287, 0.9713],\n",
      "        [0.9344, 0.0656],\n",
      "        [0.8812, 0.1188],\n",
      "        [0.8742, 0.1258]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.7443, 0.2557],\n",
      "        [0.8187, 0.1813],\n",
      "        [0.0112, 0.9888],\n",
      "        [0.8732, 0.1268],\n",
      "        [0.9435, 0.0565],\n",
      "        [0.0128, 0.9872],\n",
      "        [0.0188, 0.9812],\n",
      "        [0.7432, 0.2568],\n",
      "        [0.0162, 0.9838],\n",
      "        [0.0262, 0.9738]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.0467, 0.9533],\n",
      "        [0.9320, 0.0680],\n",
      "        [0.8418, 0.1582],\n",
      "        [0.0933, 0.9067],\n",
      "        [0.9587, 0.0413],\n",
      "        [0.6714, 0.3286],\n",
      "        [0.7343, 0.2657],\n",
      "        [0.8345, 0.1655],\n",
      "        [0.9712, 0.0288],\n",
      "        [0.1646, 0.8354]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.0761, 0.9239],\n",
      "        [0.9273, 0.0727],\n",
      "        [0.9799, 0.0201],\n",
      "        [0.0297, 0.9703],\n",
      "        [0.8487, 0.1513],\n",
      "        [0.0407, 0.9593],\n",
      "        [0.9381, 0.0619],\n",
      "        [0.9825, 0.0175],\n",
      "        [0.9533, 0.0467],\n",
      "        [0.5809, 0.4191]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.0228, 0.9772],\n",
      "        [0.9651, 0.0349],\n",
      "        [0.9586, 0.0414],\n",
      "        [0.9776, 0.0224],\n",
      "        [0.9711, 0.0289],\n",
      "        [0.9531, 0.0469],\n",
      "        [0.9030, 0.0970],\n",
      "        [0.0955, 0.9045],\n",
      "        [0.0087, 0.9913],\n",
      "        [0.0256, 0.9744]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.9836, 0.0164],\n",
      "        [0.1189, 0.8811],\n",
      "        [0.0323, 0.9677],\n",
      "        [0.9096, 0.0904],\n",
      "        [0.9776, 0.0224],\n",
      "        [0.9685, 0.0315],\n",
      "        [0.0361, 0.9639],\n",
      "        [0.0545, 0.9455],\n",
      "        [0.0359, 0.9641],\n",
      "        [0.0438, 0.9562]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.1016, 0.8984],\n",
      "        [0.9545, 0.0455],\n",
      "        [0.9767, 0.0233],\n",
      "        [0.9587, 0.0413],\n",
      "        [0.9259, 0.0741],\n",
      "        [0.0718, 0.9282],\n",
      "        [0.0771, 0.9229],\n",
      "        [0.9718, 0.0282],\n",
      "        [0.9679, 0.0321],\n",
      "        [0.9849, 0.0151]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.0728, 0.9272],\n",
      "        [0.9889, 0.0111],\n",
      "        [0.9814, 0.0186],\n",
      "        [0.9947, 0.0053],\n",
      "        [0.9952, 0.0048],\n",
      "        [0.0861, 0.9139],\n",
      "        [0.9925, 0.0075],\n",
      "        [0.9926, 0.0074],\n",
      "        [0.9935, 0.0065],\n",
      "        [0.9953, 0.0047]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.2283, 0.7717],\n",
      "        [0.9949, 0.0051],\n",
      "        [0.1747, 0.8253],\n",
      "        [0.2989, 0.7011],\n",
      "        [0.9743, 0.0257],\n",
      "        [0.9773, 0.0227],\n",
      "        [0.9777, 0.0223],\n",
      "        [0.1323, 0.8677],\n",
      "        [0.9954, 0.0046],\n",
      "        [0.1985, 0.8015]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.0821, 0.9179],\n",
      "        [0.9895, 0.0105],\n",
      "        [0.0994, 0.9006],\n",
      "        [0.9954, 0.0046],\n",
      "        [0.1855, 0.8145],\n",
      "        [0.9874, 0.0126],\n",
      "        [0.9928, 0.0072],\n",
      "        [0.9890, 0.0110],\n",
      "        [0.9893, 0.0107],\n",
      "        [0.0293, 0.9707]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.9864, 0.0136],\n",
      "        [0.0797, 0.9203],\n",
      "        [0.9896, 0.0104],\n",
      "        [0.2530, 0.7470],\n",
      "        [0.0905, 0.9095],\n",
      "        [0.9924, 0.0076],\n",
      "        [0.0707, 0.9293],\n",
      "        [0.9704, 0.0296],\n",
      "        [0.9636, 0.0364],\n",
      "        [0.9924, 0.0076]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.9838, 0.0162],\n",
      "        [0.9195, 0.0805],\n",
      "        [0.0302, 0.9698],\n",
      "        [0.1366, 0.8634],\n",
      "        [0.9924, 0.0076],\n",
      "        [0.0718, 0.9282],\n",
      "        [0.9865, 0.0135],\n",
      "        [0.0708, 0.9292],\n",
      "        [0.9616, 0.0384],\n",
      "        [0.9940, 0.0060]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  8 is 0.3401721715927124 \n",
      "batch : 0  Output : tensor([[0.0737, 0.9263],\n",
      "        [0.0434, 0.9566],\n",
      "        [0.0256, 0.9744],\n",
      "        [0.0072, 0.9928],\n",
      "        [0.9851, 0.0149],\n",
      "        [0.0306, 0.9694],\n",
      "        [0.0234, 0.9766],\n",
      "        [0.9839, 0.0161],\n",
      "        [0.9882, 0.0118],\n",
      "        [0.9909, 0.0091]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.9577, 0.0423],\n",
      "        [0.9918, 0.0082],\n",
      "        [0.0891, 0.9109],\n",
      "        [0.9829, 0.0171],\n",
      "        [0.9803, 0.0197],\n",
      "        [0.0573, 0.9427],\n",
      "        [0.0778, 0.9222],\n",
      "        [0.9926, 0.0074],\n",
      "        [0.0830, 0.9170],\n",
      "        [0.0397, 0.9603]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.1150, 0.8850],\n",
      "        [0.9561, 0.0439],\n",
      "        [0.9636, 0.0364],\n",
      "        [0.0935, 0.9065],\n",
      "        [0.9562, 0.0438],\n",
      "        [0.9837, 0.0163],\n",
      "        [0.9379, 0.0621],\n",
      "        [0.9926, 0.0074],\n",
      "        [0.9607, 0.0393],\n",
      "        [0.2990, 0.7010]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.2173, 0.7827],\n",
      "        [0.9568, 0.0432],\n",
      "        [0.9448, 0.0552],\n",
      "        [0.0672, 0.9328],\n",
      "        [0.9272, 0.0728],\n",
      "        [0.0136, 0.9864],\n",
      "        [0.9834, 0.0166],\n",
      "        [0.9800, 0.0200],\n",
      "        [0.9083, 0.0917],\n",
      "        [0.9771, 0.0229]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.0038, 0.9962],\n",
      "        [0.8196, 0.1804],\n",
      "        [0.9037, 0.0963],\n",
      "        [0.8866, 0.1134],\n",
      "        [0.7471, 0.2529],\n",
      "        [0.6658, 0.3342],\n",
      "        [0.8064, 0.1936],\n",
      "        [0.0047, 0.9953],\n",
      "        [0.0062, 0.9938],\n",
      "        [0.0070, 0.9930]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.8540, 0.1460],\n",
      "        [0.0294, 0.9706],\n",
      "        [0.0067, 0.9933],\n",
      "        [0.9353, 0.0647],\n",
      "        [0.9365, 0.0635],\n",
      "        [0.8636, 0.1364],\n",
      "        [0.0068, 0.9932],\n",
      "        [0.0066, 0.9934],\n",
      "        [0.0068, 0.9932],\n",
      "        [0.0290, 0.9710]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.0139, 0.9861],\n",
      "        [0.8247, 0.1753],\n",
      "        [0.8997, 0.1003],\n",
      "        [0.8325, 0.1675],\n",
      "        [0.7863, 0.2137],\n",
      "        [0.0075, 0.9925],\n",
      "        [0.0189, 0.9811],\n",
      "        [0.9527, 0.0473],\n",
      "        [0.7513, 0.2487],\n",
      "        [0.7572, 0.2428]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 70  Output : tensor([[0.0141, 0.9859],\n",
      "        [0.9846, 0.0154],\n",
      "        [0.9771, 0.0229],\n",
      "        [0.9630, 0.0370],\n",
      "        [0.9836, 0.0164],\n",
      "        [0.0291, 0.9709],\n",
      "        [0.9828, 0.0172],\n",
      "        [0.9512, 0.0488],\n",
      "        [0.9651, 0.0349],\n",
      "        [0.9067, 0.0933]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.0696, 0.9304],\n",
      "        [0.9873, 0.0127],\n",
      "        [0.0225, 0.9775],\n",
      "        [0.0159, 0.9841],\n",
      "        [0.9633, 0.0367],\n",
      "        [0.9846, 0.0154],\n",
      "        [0.8752, 0.1248],\n",
      "        [0.0148, 0.9852],\n",
      "        [0.9846, 0.0154],\n",
      "        [0.0174, 0.9826]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.0381, 0.9619],\n",
      "        [0.9875, 0.0125],\n",
      "        [0.0189, 0.9811],\n",
      "        [0.9882, 0.0118],\n",
      "        [0.0371, 0.9629],\n",
      "        [0.9877, 0.0123],\n",
      "        [0.9866, 0.0134],\n",
      "        [0.9938, 0.0062],\n",
      "        [0.9894, 0.0106],\n",
      "        [0.1252, 0.8748]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.9773, 0.0227],\n",
      "        [0.1160, 0.8840],\n",
      "        [0.9532, 0.0468],\n",
      "        [0.0321, 0.9679],\n",
      "        [0.0251, 0.9749],\n",
      "        [0.9938, 0.0062],\n",
      "        [0.0375, 0.9625],\n",
      "        [0.9926, 0.0074],\n",
      "        [0.9898, 0.0102],\n",
      "        [0.9925, 0.0075]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.9928, 0.0072],\n",
      "        [0.9911, 0.0089],\n",
      "        [0.0999, 0.9001],\n",
      "        [0.0120, 0.9880],\n",
      "        [0.9940, 0.0060],\n",
      "        [0.0482, 0.9518],\n",
      "        [0.9913, 0.0087],\n",
      "        [0.0188, 0.9812],\n",
      "        [0.9908, 0.0092],\n",
      "        [0.9930, 0.0070]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  9 is 0.3259392976760864 \n",
      "Time the hole training is :0.7780773639678955\n"
     ]
    }
   ],
   "source": [
    "train_model_3(model,optimizer1,nb_epochs,training_data,training_targets,mini_batch_size,net_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janjar/test/lib/python3.5/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5331, 0.4669],\n",
       "        [0.5286, 0.4714],\n",
       "        [0.5245, 0.4755],\n",
       "        [0.5264, 0.4736],\n",
       "        [0.5281, 0.4719],\n",
       "        [0.5284, 0.4716],\n",
       "        [0.5246, 0.4754],\n",
       "        [0.5336, 0.4664],\n",
       "        [0.5213, 0.4787],\n",
       "        [0.5343, 0.4657]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = model(validation_data[10:20],4)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janjar/test/lib/python3.5/site-packages/ipykernel_launcher.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 0  Output : tensor([[0.5145, 0.4855],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5147, 0.4853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.5154, 0.4846],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.5172, 0.4828],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5173, 0.4827],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5140, 0.4860],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5142, 0.4858],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5104, 0.4896],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5126, 0.4874],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5156, 0.4844],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5129, 0.4871],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5154, 0.4846],\n",
      "        [0.5131, 0.4869],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5158, 0.4842]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5109, 0.4891],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5129, 0.4871]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5136, 0.4864],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5104, 0.4896]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5122, 0.4878],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5116, 0.4883]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.5158, 0.4842],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5141, 0.4859]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.5157, 0.4843],\n",
      "        [0.5119, 0.4881],\n",
      "        [0.5134, 0.4866],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5159, 0.4841]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Time for 1 epochs is :32.11829972267151\n",
      "Loss for epoch  0 is 0.6912064552307129 \n",
      "batch : 0  Output : tensor([[0.5145, 0.4855],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5147, 0.4853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.5154, 0.4846],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.5172, 0.4828],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5173, 0.4827],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5140, 0.4860],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5142, 0.4858],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5104, 0.4896],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5126, 0.4874],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5156, 0.4844],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5129, 0.4871],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5154, 0.4846],\n",
      "        [0.5131, 0.4869],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5158, 0.4842]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5109, 0.4891],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5129, 0.4871]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5136, 0.4864],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5104, 0.4896]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5122, 0.4878],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5116, 0.4883]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.5158, 0.4842],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5141, 0.4859]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 110  Output : tensor([[0.5157, 0.4843],\n",
      "        [0.5119, 0.4881],\n",
      "        [0.5134, 0.4866],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5159, 0.4841]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  1 is 0.6912064552307129 \n",
      "batch : 0  Output : tensor([[0.5145, 0.4855],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5147, 0.4853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.5154, 0.4846],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.5172, 0.4828],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5173, 0.4827],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5140, 0.4860],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5142, 0.4858],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5104, 0.4896],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5126, 0.4874],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5156, 0.4844],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5129, 0.4871],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5154, 0.4846],\n",
      "        [0.5131, 0.4869],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5158, 0.4842]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5109, 0.4891],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5129, 0.4871]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5136, 0.4864],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5104, 0.4896]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5122, 0.4878],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5116, 0.4883]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.5158, 0.4842],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5141, 0.4859]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.5157, 0.4843],\n",
      "        [0.5119, 0.4881],\n",
      "        [0.5134, 0.4866],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5159, 0.4841]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  2 is 0.6912064552307129 \n",
      "batch : 0  Output : tensor([[0.5145, 0.4855],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5147, 0.4853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.5154, 0.4846],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.5172, 0.4828],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5173, 0.4827],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5140, 0.4860],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5142, 0.4858],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5104, 0.4896],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5126, 0.4874],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5156, 0.4844],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5129, 0.4871],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5154, 0.4846],\n",
      "        [0.5131, 0.4869],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5158, 0.4842]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5109, 0.4891],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5129, 0.4871]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5136, 0.4864],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5104, 0.4896]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5122, 0.4878],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5116, 0.4883]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch : 100  Output : tensor([[0.5158, 0.4842],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5141, 0.4859]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.5157, 0.4843],\n",
      "        [0.5119, 0.4881],\n",
      "        [0.5134, 0.4866],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5159, 0.4841]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  3 is 0.6912064552307129 \n",
      "batch : 0  Output : tensor([[0.5145, 0.4855],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5147, 0.4853]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "batch : 10  Output : tensor([[0.5154, 0.4846],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5108, 0.4892],\n",
      "        [0.5109, 0.4891],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5151, 0.4849],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "batch : 20  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5168, 0.4832],\n",
      "        [0.5172, 0.4828],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5166, 0.4834],\n",
      "        [0.5173, 0.4827],\n",
      "        [0.5128, 0.4872]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 0, 0, 0, 0, 1])\n",
      "batch : 30  Output : tensor([[0.5140, 0.4860],\n",
      "        [0.5138, 0.4862],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5125, 0.4875],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5142, 0.4858],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5122, 0.4878]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 1, 0, 1, 0, 0, 0, 0])\n",
      "batch : 40  Output : tensor([[0.5104, 0.4896],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5153, 0.4847],\n",
      "        [0.5107, 0.4893],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5146, 0.4854],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5126, 0.4874],\n",
      "        [0.5135, 0.4865],\n",
      "        [0.5148, 0.4852]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n",
      "batch : 50  Output : tensor([[0.5152, 0.4848],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5156, 0.4844],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5147, 0.4853],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5111, 0.4889],\n",
      "        [0.5155, 0.4845]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 1, 0, 0, 0, 1, 1, 1, 1])\n",
      "batch : 60  Output : tensor([[0.5129, 0.4871],\n",
      "        [0.5148, 0.4852],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5154, 0.4846],\n",
      "        [0.5131, 0.4869],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5158, 0.4842]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])\n",
      "batch : 70  Output : tensor([[0.5109, 0.4891],\n",
      "        [0.5145, 0.4855],\n",
      "        [0.5136, 0.4864],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5116, 0.4884],\n",
      "        [0.5123, 0.4877],\n",
      "        [0.5122, 0.4878],\n",
      "        [0.5174, 0.4826],\n",
      "        [0.5114, 0.4886],\n",
      "        [0.5129, 0.4871]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "batch : 80  Output : tensor([[0.5136, 0.4864],\n",
      "        [0.5193, 0.4807],\n",
      "        [0.5133, 0.4867],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5129, 0.4871],\n",
      "        [0.5128, 0.4872],\n",
      "        [0.5171, 0.4829],\n",
      "        [0.5157, 0.4843],\n",
      "        [0.5167, 0.4833],\n",
      "        [0.5104, 0.4896]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 1, 0, 0, 0, 1, 0, 1])\n",
      "batch : 90  Output : tensor([[0.5122, 0.4878],\n",
      "        [0.5152, 0.4848],\n",
      "        [0.5150, 0.4850],\n",
      "        [0.5115, 0.4885],\n",
      "        [0.5159, 0.4841],\n",
      "        [0.5143, 0.4857],\n",
      "        [0.5158, 0.4842],\n",
      "        [0.5139, 0.4861],\n",
      "        [0.5124, 0.4876],\n",
      "        [0.5116, 0.4883]], grad_fn=<SoftmaxBackward>) Targets: tensor([1, 0, 1, 0, 1, 0, 0, 0, 0, 1])\n",
      "batch : 100  Output : tensor([[0.5158, 0.4842],\n",
      "        [0.5141, 0.4859],\n",
      "        [0.5137, 0.4863],\n",
      "        [0.5162, 0.4838],\n",
      "        [0.5144, 0.4856],\n",
      "        [0.5086, 0.4914],\n",
      "        [0.5165, 0.4835],\n",
      "        [0.5120, 0.4880],\n",
      "        [0.5161, 0.4839],\n",
      "        [0.5141, 0.4859]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 1, 0, 1, 1, 0, 1, 0, 0, 0])\n",
      "batch : 110  Output : tensor([[0.5157, 0.4843],\n",
      "        [0.5119, 0.4881],\n",
      "        [0.5134, 0.4866],\n",
      "        [0.5175, 0.4825],\n",
      "        [0.5140, 0.4860],\n",
      "        [0.5127, 0.4873],\n",
      "        [0.5130, 0.4870],\n",
      "        [0.5163, 0.4837],\n",
      "        [0.5098, 0.4902],\n",
      "        [0.5159, 0.4841]], grad_fn=<SoftmaxBackward>) Targets: tensor([0, 0, 1, 1, 0, 1, 0, 1, 0, 0])\n",
      "Loss for epoch  4 is 0.6912064552307129 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-7781cdbeff5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mws_ch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-13cafce4d6bf>\u001b[0m in \u001b[0;36mtrain_model_3\u001b[0;34m(model, optimizer, nb_epochs, train_input, train_target, mini_batch_size, n)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;31m#print(\"Shape of the input of the model:\",train_input.narrow(0, b, mini_batch_size).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-119-c7e0a6f81307>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, n)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m129\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/test/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model_3(model1,optimizer1,nb_epochs,training_data,training_targets,mini_batch_size,ws_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semesterproject",
   "language": "python",
   "name": "semesterproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
